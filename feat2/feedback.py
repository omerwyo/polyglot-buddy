import language_tool_python
from langchain.prompts import PromptTemplate
import json
import requests
from sentence_similarity import sentence_similarity


# from llama_cpp import Llama
# llm = Llama(model_path="/common/home/projectgrps/CS425/CS425G6/llama.cpp/models/llama-2-7b/llama-2-7b.Q4_K_M.gguf", n_ctx=2048)
# def get_from_llama_cpp(prompt):
#     output = llm(prompt)

#     print(output)

#     return output


feedback_template = """
Given a passage in {language}, a question, a correct answer and a my response to the question, come up with some feedback for me.

Apart from giving you the above information, you are given some analyis on my grammar as generated by a rule-based grammar checker, as well as the semantic similarity score between the correct answer and my answer by a SentenceTransformer model.

Adapt the feedback from the grammar checker, and your own thoughts based on the passage and how my answer might/might not differ from the correct answer. Take into account the semantic similarity of the correct answer and my answer.

Give the feedback in English, as that is my native language, and quote my mistakes in {language}. Below is the information you can work with.

Passage: {passage}
Question: {question}
User's Answer: {user_answer}
Correct Answer: {correct_answer}
Semantic Similarity Score: {semantic_similarity_score}
Grammar Check Analysis: 
{grammar_matches}

Do not prompt me to ask you any additional questions, and do not thank me for providing you with information. Simply provide feedback.
"""

feedback_prompt_template = PromptTemplate(
    input_variables=["language", "passage", "question", "user_answer", "correct_answer", "semantic_similarity_score", "grammar_matches"],
    template=feedback_template
)

language_tool_map = {
    'Spanish': language_tool_python.LanguageTool('es'),
    'German': language_tool_python.LanguageTool('de'),
    'English': language_tool_python.LanguageTool('en'),
    'Italian': language_tool_python.LanguageTool('it'),
    'French': language_tool_python.LanguageTool('fr'),
    # 'Chinese': 'zh-CN',
}

def get_from_ollama(prompt, model="llama2"):
    url = "http://localhost:11434/api/generate"

    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "temperature": 0.8,
    }

    headers = {"Content-Type": "application/json"}
    response = requests.post(url, data=json.dumps(data), headers=headers)
    generated_dict = response.json()

    # print(f"Total Duration: {generated_dict['total_duration']}")
    # print(f"Eval Duration: {generated_dict['eval_duration']}")
    # print(f"Prompt Eval Duration: {generated_dict['prompt_eval_duration']}")

    # Remove redundant keys
    redundant_keys = ['total_duration', 'context', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration', 'model', 'created_at']
    for key in redundant_keys:
        generated_dict.pop(key, None)

    return generated_dict['response'] if generated_dict['done'] == True else "Unable to provide feedback :/"

def get_from_llama_cpp(prompt):
    url = "http://localhost:8912/completion"

    print(prompt)

    data = {
        "prompt": prompt,
        "stream": False,
        "temperature": 0.8,
    }

    headers = {"Content-Type": "application/json"}
    response = requests.post(url, data=json.dumps(data), headers=headers)
    generated_dict = response.json()

    print(generated_dict)

    return generated_dict['content']

def get_grammar_matches(language, user_answer):
    tool = language_tool_map[language]
    return tool.check(user_answer)

def compare_sentences(sentence_1=str, sentence_2=str, model_name="sentence-transformers/all-MiniLM-L6-v2", embedding_type="cls_token_embedding", metric="cosine") -> str:
    model = sentence_similarity(model_name=model_name, embedding_type=embedding_type)
    score = model.get_score(sentence_1, sentence_2, metric=metric)
    return score

def get_answer_feedback(language, question_id, user_answer):
    matches = get_grammar_matches(language, user_answer)
    
    filename = f'/common/home/projectgrps/CS425/CS425G6/polyglot-buddy/feat2/clean/{language}_short_answer.json'

    with open(filename, 'r') as file:
        data = json.load(file)
        
    passage_question_answer = data[question_id - 1]
    
    # Get semantic similarity score between user's answer and our answer
    semantic_similarity_score = compare_sentences(user_answer, passage_question_answer['answer'])
    print(f"Semantic Similarity score: {semantic_similarity_score}")

    prompt = feedback_prompt_template.format(
        language = language,
        passage = passage_question_answer['passage'],
        question = passage_question_answer['question'],
        user_answer = user_answer,
        correct_answer = passage_question_answer['answer'],
        semantic_similarity_score = semantic_similarity_score,
        grammar_matches = ''.join([str(match) for match in matches]),
    )

    # return get_from_llama_cpp(prompt)
    return get_from_ollama(prompt, model="llama2:13b")